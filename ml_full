pip install numpy pandas matplotlib seaborn scikit-learn
# ==============================
# MACHINE LEARNING PRACTICAL
# COMPLETE END-TO-END CODE
# ==============================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import (
    load_iris,
    load_wine,
    load_breast_cancer,
    load_diabetes,
    load_digits
)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA

from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    mean_squared_error,
    r2_score
)

# ==============================
# 1. DATA LOADING FUNCTION
# ==============================
def load_dataset(name):
    if name == "iris":
        data = load_iris()
    elif name == "wine":
        data = load_wine()
    elif name == "cancer":
        data = load_breast_cancer()
    elif name == "digits":
        data = load_digits()
    elif name == "diabetes":
        data = load_diabetes()
    else:
        return None

    X = pd.DataFrame(data.data)
    y = pd.Series(data.target)
    return X, y


# ==============================
# 2. EDA FUNCTION
# ==============================
def perform_eda(X):
    print("\nEDA SUMMARY")
    print(X.describe())
    print("\nMissing Values:\n", X.isnull().sum())


# ==============================
# 3. FEATURE ENGINEERING
# ==============================
def feature_engineering(X):
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    return X_scaled


# ==============================
# 4. KNN CLASSIFIER
# ==============================
def knn_model(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    model = KNeighborsClassifier(n_neighbors=5)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print("\nKNN Accuracy:", accuracy_score(y_test, y_pred))


# ==============================
# 5. LINEAR REGRESSION
# ==============================
def linear_regression_model():
    X, y = load_dataset("diabetes")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    model = LinearRegression()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print("\nLinear Regression MSE:", mean_squared_error(y_test, y_pred))
    print("Linear Regression R2:", r2_score(y_test, y_pred))


# ==============================
# 6. LOGISTIC REGRESSION
# ==============================
def logistic_regression_model(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    model = LogisticRegression(max_iter=200)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print("\nLogistic Regression Accuracy:", accuracy_score(y_test, y_pred))


# ==============================
# 7. SUPPORT VECTOR MACHINE
# ==============================
def svm_model(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    model = SVC(kernel='rbf')
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print("\nSVM Accuracy:", accuracy_score(y_test, y_pred))


# ==============================
# 8. NAIVE BAYES
# ==============================
def naive_bayes_model(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    model = GaussianNB()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print("\nNaive Bayes Accuracy:", accuracy_score(y_test, y_pred))


# ==============================
# 9. DECISION TREE
# ==============================
def decision_tree_model(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    model = DecisionTreeClassifier(max_depth=5)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print("\nDecision Tree Accuracy:", accuracy_score(y_test, y_pred))


# ==============================
# 10. RANDOM FOREST
# ==============================
def random_forest_model(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    model = RandomForestClassifier(n_estimators=100)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print("\nRandom Forest Accuracy:", accuracy_score(y_test, y_pred))


# ==============================
# 11. ADABOOST
# ==============================
def adaboost_model(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    model = AdaBoostClassifier(n_estimators=50)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print("\nAdaBoost Accuracy:", accuracy_score(y_test, y_pred))


# ==============================
# MAIN EXECUTION
# ==============================
datasets = ["iris", "wine", "cancer", "digits"]

for name in datasets:
    print("\n==============================")
    print("DATASET:", name.upper())
    print("==============================")

    X, y = load_dataset(name)
    perform_eda(X)
    X_scaled = feature_engineering(X)

    knn_model(X_scaled, y)
    logistic_regression_model(X_scaled, y)
    svm_model(X_scaled, y)
    naive_bayes_model(X_scaled, y)
    decision_tree_model(X_scaled, y)
    random_forest_model(X_scaled, y)
    adaboost_model(X_scaled, y)

# Linear Regression (separate dataset)
linear_regression_model()

print("\nALL PRACTICAL EXPERIMENTS COMPLETED SUCCESSFULLY")
